# This file was generated by Codebase-Generator, do not edit directly
"""
LLM component: unified interface to various LLM providers and optional MCP servers.
"""

import logging
import os
import time
from typing import Optional, List, Type, Union

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServer

from recipe_executor.llm_utils.azure_openai import get_azure_openai_model
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.anthropic import AnthropicModel

# default Ollama endpoint env var
_OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


def get_model(model_id: str, logger: logging.Logger) -> Union[OpenAIModel, AnthropicModel]:  # may expand return types
    """
    Initialize an LLM model based on a standardized model_id string.
    Format: provider/model_name or provider/model_name/deployment_name
    Supported: openai, azure, anthropic, ollama
    """
    parts = model_id.split("/", 2)
    if len(parts) < 2:
        raise ValueError(f"Invalid model_id '{model_id}'; expected 'provider/model_name' format")
    provider, model_name = parts[0], parts[1]
    deployment = parts[2] if len(parts) == 3 else None

    if provider == "azure":
        # Azure OpenAI: may include deployment name or default to model_name
        deployment_name = deployment or model_name
        try:
            return get_azure_openai_model(
                logger=logger,
                model_name=model_name,
                deployment_name=deployment_name,
            )
        except Exception as e:
            raise ValueError(f"Azure OpenAI model initialization failed: {e}")

    elif provider == "openai":
        # OpenAI (default provider)
        return OpenAIModel(model_name)

    elif provider == "anthropic":
        return AnthropicModel(model_name)

    elif provider == "ollama":
        # Ollama uses OpenAIModel with custom base_url
        base_url = f"{_OLLAMA_BASE_URL.rstrip('/')}/v1"
        provider_obj = OpenAIProvider(base_url=base_url)
        return OpenAIModel(model_name, provider=provider_obj)

    else:
        raise ValueError(f"Unsupported LLM provider '{provider}' in '{model_id}'")


class LLM:
    """
    Unified interface for LLM providers and optional MCP servers.
    """

    def __init__(
        self,
        logger: logging.Logger,
        model: str = "openai/gpt-4o",
        max_tokens: Optional[int] = None,
        mcp_servers: Optional[List[MCPServer]] = None,
    ):
        """
        Initialize the LLM component.
        Args:
            logger: Logger for logging messages.
            model: Model identifier 'provider/model_name[/deployment_name]'.
            max_tokens: Maximum tokens for response.
            mcp_servers: Optional list of MCPServer instances.
        """
        self.logger: logging.Logger = logger
        # allow environment override
        env_model = os.getenv("DEFAULT_MODEL")
        self.model_id: str = env_model if env_model is not None else model
        self.max_tokens: Optional[int] = max_tokens
        self.mcp_servers: List[MCPServer] = mcp_servers or []

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        output_type: Type[Union[str, BaseModel]] = str,
        mcp_servers: Optional[List[MCPServer]] = None,
    ) -> Union[str, BaseModel]:
        """
        Generate an output from the LLM based on the prompt.
        """
        # decide parameters
        model_id = model or self.model_id
        max_toks = max_tokens if max_tokens is not None else self.max_tokens
        servers = mcp_servers if mcp_servers is not None else self.mcp_servers

        # parse provider and log info
        provider = model_id.split("/", 1)[0]
        self.logger.info(f"LLM generate: provider={provider}, model={model_id}")

        # init model
        try:
            llm_model = get_model(model_id, self.logger)
        except Exception as e:
            self.logger.error(f"Invalid model configuration: {e}")
            raise

        # build model_settings
        model_settings = {}
        if max_toks is not None:
            model_settings["max_tokens"] = max_toks

        # create agent
        agent = Agent(
            model=llm_model,
            output_type=output_type,
            mcp_servers=servers,
            model_settings=model_settings or None,
        )

        # debug log request
        self.logger.debug(
            f"LLM request payload: prompt={prompt!r}, model_settings={model_settings}, mcp_servers={servers}"
        )

        # run
        start = time.monotonic()
        async with agent.run_mcp_servers():
            result = await agent.run(prompt)
        elapsed = time.monotonic() - start

        # usage
        usage = result.usage()
        self.logger.info(
            f"LLM completed in {elapsed:.2f}s, tokens: request={usage.request_tokens},"
            f" response={usage.response_tokens}, total={usage.total_tokens}"
        )
        self.logger.debug(f"LLM full result payload: {result}")

        return result.output
