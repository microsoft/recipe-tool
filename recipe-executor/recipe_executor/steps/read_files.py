# This file was generated by Codebase-Generator, do not edit directly
import glob
import json
import logging
import os
from typing import Any, Dict, List, Union

import yaml
from recipe_executor.steps.base import BaseStep, StepConfig
from recipe_executor.protocols import ContextProtocol
from recipe_executor.utils.templates import render_template


def _parse_content(path: str, raw: str) -> Any:
    """
    Deserialize JSON/YAML content when applicable, otherwise return raw text.
    """
    lower = path.lower()
    try:
        if lower.endswith(".json"):
            return json.loads(raw)
        if lower.endswith(".yaml") or lower.endswith(".yml"):
            return yaml.safe_load(raw)
    except Exception as e:
        # If deserialization fails, fallback to raw text
        logging.getLogger(__name__).warning(f"Failed to parse file {path} as JSON/YAML: {e}, returning raw text")
    return raw


class ReadFilesConfig(StepConfig):
    """
    Configuration for ReadFilesStep.

    Fields:
        path (Union[str, List[str]]): Path, comma-separated string, or list of paths to the file(s) to read.
        content_key (str): Name to store the file content in context.
        optional (bool): Whether to continue if a file is not found.
        merge_mode (str): How to handle multiple files' content: 'concat' or 'dict'.
    """

    path: Union[str, List[str]]
    content_key: str
    optional: bool = False
    merge_mode: str = "concat"


class ReadFilesStep(BaseStep[ReadFilesConfig]):
    """
    Step to read one or more files and store their content in the context.
    """

    def __init__(self, logger: logging.Logger, config: Dict[str, Any]) -> None:
        # Validate config with Pydantic
        cfg = ReadFilesConfig.model_validate(config)
        super().__init__(logger, cfg)

    async def execute(self, context: ContextProtocol) -> None:
        # Render content key template
        content_key = render_template(self.config.content_key, context)

        # Resolve and normalize paths
        raw_path = self.config.path
        paths: List[str] = []
        # Single string case
        if isinstance(raw_path, str):
            rendered = render_template(raw_path, context)
            # Split comma-separated
            if "," in rendered:
                parts = [p.strip() for p in rendered.split(",") if p.strip()]
            else:
                parts = [rendered]
        else:
            # List of strings
            parts = []
            for p in raw_path:
                rendered = render_template(p, context)
                parts.extend([r.strip() for r in (rendered.split(",") if "," in rendered else [rendered]) if r.strip()])

        # Expand globs and normalize
        for item in parts:
            pattern = os.path.expandvars(os.path.expanduser(item))
            matches = glob.glob(pattern, recursive=True)
            if matches:
                paths.extend(matches)
            else:
                # treat as literal missing path
                paths.append(pattern)

        # Deduplicate while preserving order
        seen = set()
        resolved: List[str] = []
        for p in paths:
            if p not in seen:
                seen.add(p)
                resolved.append(p)
        paths = resolved

        # Determine missing vs existing
        missing = [p for p in paths if not os.path.isfile(p)]
        existing = [p for p in paths if os.path.isfile(p)]

        # Error on missing if not optional
        if missing and not self.config.optional:
            if len(paths) == 1:
                raise FileNotFoundError(f"File not found: {missing[0]}")
            raise FileNotFoundError(f"Files not found: {missing}")

        # Read each existing file
        contents: List[Any] = []
        file_map: Dict[str, Any] = {}
        for p in existing:
            self.logger.debug(f"Reading file at path: {p}")
            try:
                with open(p, encoding="utf-8") as f:
                    raw = f.read()
            except Exception:
                raise
            parsed = _parse_content(p, raw)
            self.logger.info(f"Successfully read file: {p}")
            contents.append(parsed)
            file_map[p] = parsed

        # Build result according to count and merge_mode
        if len(existing) == 1:
            result = contents[0]
        else:
            if self.config.merge_mode == "dict":
                result = file_map
            elif self.config.merge_mode == "concat":
                # concatenate with filename headers
                parts_out: List[str] = []
                for p, c in file_map.items():
                    text = c if isinstance(c, str) else json.dumps(c)
                    parts_out.append(f"--- {p} ---\n{text}")
                result = "\n".join(parts_out)
            else:
                raise ValueError(f"Unknown merge_mode: {self.config.merge_mode}")

        # Store in context, for optional single missing, store empty according to spec
        if not existing and self.config.optional:
            if self.config.merge_mode == "dict":
                result = {}
            else:
                result = ""

        context[content_key] = result
        self.logger.info(f"Stored content under key: {content_key}")
