# This file was generated by Codebase-Generator, do not edit directly
import logging
from typing import Any, Dict, List, Optional, Union, Type

from pydantic import BaseModel

from recipe_executor.models import FileSpec
from recipe_executor.protocols import ContextProtocol
from recipe_executor.steps.base import BaseStep, StepConfig
from recipe_executor.llm_utils.llm import LLM
from recipe_executor.llm_utils.mcp import get_mcp_server
from recipe_executor.utils.models import json_object_to_pydantic_model
from recipe_executor.utils.templates import render_template


class LLMGenerateConfig(StepConfig):
    """
    Config for LLMGenerateStep.

    Fields:
        prompt: The prompt to send to the LLM (templated beforehand).
        model: The model identifier to use (provider/model_name format).
        max_tokens: The maximum number of tokens for the LLM response.
        mcp_servers: List of MCP servers for access to tools.
        output_format: The format of the LLM output (text, files, or JSON schema).
        output_key: The name under which to store the LLM output in context.
    """

    prompt: str
    model: str = "openai/gpt-4o"
    max_tokens: Optional[Union[str, int]] = None
    mcp_servers: Optional[List[Dict[str, Any]]] = None
    output_format: Union[str, Dict[str, Any], List[Any]]
    output_key: str = "llm_output"


class FileSpecCollection(BaseModel):
    files: List[FileSpec]


class LLMGenerateStep(BaseStep[LLMGenerateConfig]):
    def __init__(self, logger: logging.Logger, config: Dict[str, Any]) -> None:
        super().__init__(logger, LLMGenerateConfig(**config))

    async def execute(self, context: ContextProtocol) -> None:
        # Render templated values
        prompt = render_template(self.config.prompt, context)
        model = render_template(self.config.model, context)
        output_key = render_template(self.config.output_key, context)

        # Process max_tokens
        max_tokens: Optional[int] = None
        if self.config.max_tokens is not None:
            rendered = render_template(str(self.config.max_tokens), context)
            try:
                max_tokens = int(rendered)
            except ValueError:
                raise ValueError(f"Invalid max_tokens value: {rendered}")

        # Collect MCP server configurations: from context config and step config
        context_mcp_cfgs = context.get_config().get("mcp_servers", []) or []
        step_mcp_cfgs: List[Dict[str, Any]] = []
        if self.config.mcp_servers:
            for cfg in self.config.mcp_servers:
                rendered_cfg: Dict[str, Any] = {}
                for k, v in cfg.items():
                    if isinstance(v, str):
                        rendered_cfg[k] = render_template(v, context)
                    else:
                        rendered_cfg[k] = v
                step_mcp_cfgs.append(rendered_cfg)
        all_mcp_cfgs = list(context_mcp_cfgs) + step_mcp_cfgs

        mcp_servers = [get_mcp_server(logger=self.logger, config=cfg) for cfg in all_mcp_cfgs]

        # Initialize LLM client
        llm = LLM(logger=self.logger, model=model, max_tokens=max_tokens, mcp_servers=mcp_servers)

        # Determine output format and types
        of = self.config.output_format
        output_type: Union[Type[str], Type[BaseModel]]
        is_list = False
        is_files = False
        if isinstance(of, str):
            if of == "text":
                output_type = str  # type: ignore
            elif of == "files":
                output_type = FileSpecCollection  # type: ignore
                is_files = True
            else:
                raise ValueError(f"Unsupported output_format string: {of}")
        elif isinstance(of, dict):
            # JSON schema for object
            output_type = json_object_to_pydantic_model(of, model_name="LLMOutputModel")  # type: ignore
        elif isinstance(of, list):
            # List schema: wrap in object
            is_list = True
            if len(of) != 1:
                raise ValueError("output_format list must contain a single item schema")
            item_schema = of[0]
            wrapper_schema: Dict[str, Any] = {
                "type": "object",
                "properties": {"items": item_schema},
                "required": ["items"],
            }
            output_type = json_object_to_pydantic_model(wrapper_schema, model_name="LLMListOutputModel")  # type: ignore
        else:
            raise ValueError("Invalid output_format type")

        # Call LLM
        self.logger.debug(
            f"Calling LLM generate: model={model}, prompt={prompt}, max_tokens={max_tokens}, output_format={of}"
        )
        try:
            result = await llm.generate(
                prompt=prompt,
                model=model,
                max_tokens=max_tokens,
                output_type=output_type,
                mcp_servers=mcp_servers,
            )
        except Exception as e:
            self.logger.error(f"LLM generate failed: {e}")
            raise

        # Store result in context
        if is_files:
            # result is FileSpecCollection
            files = result.files  # type: ignore
            context[output_key] = files
        elif is_list:
            data = result.model_dump()  # type: ignore
            context[output_key] = data.get("items", [])
        elif isinstance(of, dict):
            # object schema
            data = result.model_dump()  # type: ignore
            context[output_key] = data
        else:
            # text
            context[output_key] = result  # type: ignore
